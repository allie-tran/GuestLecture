{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Environment\n",
    "We need to clone the repository and install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab only\n",
    "!git clone https://github.com/allie-tran/GuestLecture.git\n",
    "%cd GuestLecture/\n",
    "\n",
    "# Installing CLIP\n",
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some utility functions to show the results for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils to show images\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "data_dir = \"coco/val2017\"\n",
    "\n",
    "def image_to_path(image_id):\n",
    "    return f\"{data_dir}/{image_id}\"\n",
    "\n",
    "def show_images(images, shuffle=True):\n",
    "    if shuffle:\n",
    "        images = random.sample(list(images), 9)\n",
    "    else:\n",
    "        images = images[:9]\n",
    "    images = [image_to_path(image) for image in images]\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = mpimg.imread(images[i])\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection example using Detr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code modified from https://huggingface.co/facebook/detr-resnet-50\n",
    "\"\"\"\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# load an image\n",
    "ImageId = \"000000002157.jpg\"\n",
    "image = Image.open(ImageId)\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\", revision=\"no_timm\"\n",
    ")\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\", revision=\"no_timm\"\n",
    ")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs, target_sizes=target_sizes, threshold=0.9\n",
    ")[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding-based Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the model. Make sure you choose GPU in the Runtime settings.\n",
    "CPU will be too slow for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"It might take a while to download the model when running for the first time.\")\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare image and text similarity\n",
    "We will use the CLIP model to encdoe the image and text and then compare the similarity between them.\n",
    "\n",
    "Note: cosine similarity can be calculated using the formula:\n",
    "$$\n",
    "\\text{similarity} = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\n",
    "$$\n",
    "\n",
    "Where A and B are the vectors to be compared.\n",
    "Hence we need to normalize the vectors before calculating the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"000000002157.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"I am ordering a coffee at a cafe\", \"I am hiking in the mountains\"]).to(device)\n",
    "\n",
    "# Encode the image and the text\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "# Calculate the similarity\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).cpu().numpy()\n",
    "\n",
    "print(\"Similarity between the image and the first sentence:\", similarity[0, 0])\n",
    "print(\"Similarity between the image and the second sentence:\", similarity[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MSCOCO 2017 validation images\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a directory\n",
    "download_dir = Path(\"coco\")\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download the images\n",
    "url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "target = download_dir / \"val2017.zip\"\n",
    "\n",
    "print(\"Downloading...\")\n",
    "if not target.exists():\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(target, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    os.system(f\"unzip {target} -d {download_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "from tqdm.auto import tqdm \n",
    "# Load the images\n",
    "images = os.listdir(\"coco/val2017\")\n",
    "print(f\"Found {len(images)} images.\")\n",
    "# Encode the images\n",
    "image_features = []\n",
    "valid_images = []\n",
    "print(\"Encoding images. This might take a while...\")\n",
    "for image in tqdm(images):\n",
    "    try:\n",
    "        image_path = f\"coco/val2017/{image}\"\n",
    "        img = Image.open(image_path)\n",
    "        img = preprocess(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = model.encode_image(img)\n",
    "            # Normalize the features for cosine similarity (not required, but can help)\n",
    "            feature = feature / feature.norm(dim=-1, keepdim=True) \n",
    "            image_features.append(feature.cpu().numpy())\n",
    "            valid_images.append(image)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Stack the features\n",
    "image_features = np.stack(image_features) # (N, 768)\n",
    "print(f\"Encoded {len(image_features)} images.\")\n",
    "\n",
    "# Save the features\n",
    "np.save(\"image_features.npy\", image_features)\n",
    "np.save(\"images.npy\", valid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features\n",
    "image_features = np.load(\"image_features.npy\")\n",
    "valid_images = np.load(\"images.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am ordering a coffee at a cafe\"\n",
    "text = clip.tokenize([text]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy()\n",
    "\n",
    "# Calculate the similarity\n",
    "similarity = (100.0 * image_features @ text_features.T).ravel()\n",
    "\n",
    "# Show the images with the highest similarity\n",
    "idx = similarity.argsort()[-9:][::-1]\n",
    "show_images(np.array(valid_images)[idx], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query by example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = \"000000002157.jpg\"\n",
    "image = Image.open(example_image)\n",
    "image = preprocess(image).to(device).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    example_features = model.encode_image(image)\n",
    "    example_features /= example_features.norm(dim=-1, keepdim=True)\n",
    "    example_features = example_features.cpu().numpy()\n",
    "\n",
    "# Calculate the similarity\n",
    "similarity = (100.0 * image_features @ example_features.T).squeeze()\n",
    "similarity = similarity.argsort()[-9:][::-1]\n",
    "show_images(np.array(valid_images)[similarity], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duyen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
