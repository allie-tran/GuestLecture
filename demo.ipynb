{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utillity functions\n",
    "data_dir = \"/mnt/DATA/duyen/highres/LSC23/LSC23_highres_images\"\n",
    "\n",
    "def image_to_path(image_id):\n",
    "    \"\"\"image_id = %Y%m%d_xxx\n",
    "    path = %DIR/%Y/%m/%d/image_id.jpg\n",
    "    \"\"\"\n",
    "    return f\"{data_dir}/{image_id[:6]}/{image_id[6:8]}/{image_id}\"\n",
    "\n",
    "def show_images(images, shuffle=True):\n",
    "    if shuffle:\n",
    "        images = random.sample(list(images), 9)\n",
    "    else:\n",
    "        images = images[:9]\n",
    "    images = [image_to_path(image) for image in images]\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = mpimg.imread(images[i])\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('metadata.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on location\n",
    "location = \"Dublin\"\n",
    "filtered_images = df[df[\"city\"].str.contains(location, na=False, case=False)]\n",
    "show_images(filtered_images[\"ImageID\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection example using Detr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code modified from https://huggingface.co/facebook/detr-resnet-50\n",
    "\"\"\"\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# load an image\n",
    "ImageId = \"20160808_111247_000.jpeg\"\n",
    "image = Image.open(ImageId)\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\", revision=\"no_timm\"\n",
    ")\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\", revision=\"no_timm\"\n",
    ")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs, target_sizes=target_sizes, threshold=0.9\n",
    ")[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The csv file already contains the tags for each image\n",
    "df[\"Tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on tags\n",
    "tags = [\"building\", \"road\"]\n",
    "filtered_images = df[df[\"Tags\"].str.contains(\"|\".join(tags), na=False, case=False)]\n",
    "show_images(filtered_images[\"ImageID\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also other metadata\n",
    "df[[\"OCR\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on OCR\n",
    "ocr = [\"DCU\"]\n",
    "filtered_images = df[df[\"OCR\"].str.contains(\"|\".join(ocr), na=False, case=False)]\n",
    "\n",
    "exclude_tags = [\"screen\", \"laptop\", \"monitor\"]\n",
    "filtered_images = filtered_images[~filtered_images[\"Tags\"].str.contains(\"|\".join(exclude_tags), na=False, case=False)]\n",
    "\n",
    "show_images(filtered_images[\"ImageID\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Caption\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the TfidfVectorizer on the caption\n",
    "X = vectorizer.fit_transform(df[\"Caption\"].dropna())\n",
    "\n",
    "# Search for images based on the caption\n",
    "query = \"I am ordering a coffee at a cafe\"\n",
    "query_vector = vectorizer.transform([query])\n",
    "results = (X @ query_vector.T).toarray().ravel()\n",
    "best = results.argsort()[-9:][::-1]\n",
    "show_images(df.iloc[best][\"ImageID\"].values, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare image and text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"20160808_111247_000.jpeg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"I am ordering a coffee at a cafe\", \"I am hiking in the mountains\"]).to(device)\n",
    "\n",
    "# Encode the image and the text\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "# Calculate the similarity\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(similarity.round(2))\n",
    "print(\"The first sentence is more similar to the image than the second one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# Let's encode all the images first\n",
    "image_features = {}\n",
    "print(\"This might take a while...\")\n",
    "all_images = df[\"ImageID\"].values\n",
    "# all_images = random.sample(list(all_images), 1000)\n",
    "for image_id in tqdm(all_images):\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_to_path(image_id))).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = model.encode_image(image)\n",
    "            feature = feature / feature.norm(dim=-1, keepdim=True)\n",
    "            image_features[image_id] = feature.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the features so we can load them later\n",
    "torch.save(image_features, \"image_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# image_features = torch.load(\"image_features.pt\")\n",
    "# all_images = list(image_features.keys())\n",
    "# image_features = np.concatenate(list(image_features.values()), axis=0)\n",
    "image_features = np.load(\"/mnt/DATA/duyen/highres/LSC23/ViT-L-14-336_openai_nonorm/features.npy\")\n",
    "image_features = image_features / np.linalg.norm(image_features, axis=1, keepdims=True)\n",
    "all_images = pd.read_csv(\"/mnt/DATA/duyen/highres/LSC23/ViT-L-14-336_openai_nonorm/photo_ids.csv\")[\"photo_id\"].tolist()\n",
    "all_images = [image.split(\"/\")[-1] for image in all_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am ordering a sandwich at a cafe at the till\"\n",
    "\n",
    "text = clip.tokenize([text]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy()\n",
    "\n",
    "# Calculate the similarity\n",
    "similarity = (100.0 * image_features @ text_features.T).ravel()\n",
    "\n",
    "# Show the images with the highest similarity\n",
    "idx = similarity.argsort()[-9:][::-1]\n",
    "show_images(np.array(all_images)[idx], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query by example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = \"20160808_111247_000.jpeg\"\n",
    "image = Image.open(example_image)\n",
    "image = preprocess(image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    example_features = model.encode_image(image)\n",
    "    example_features /= example_features.norm(dim=-1, keepdim=True)\n",
    "    example_features = example_features.cpu().numpy()\n",
    "\n",
    "# Calculate the similarity\n",
    "similarity = (100.0 * image_features @ example_features.T).squeeze()\n",
    "similarity = similarity.argsort()[-9:][::-1]\n",
    "show_images(np.array(all_images)[similarity], shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duyen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
